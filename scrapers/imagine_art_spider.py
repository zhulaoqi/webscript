"""
Imagine.art Spider - åŸºäº Scrapy æ¡†æ¶
ä¸“ä¸šçˆ¬è™«å®ç°ï¼Œç›´æ¥è°ƒç”¨ API
"""
import scrapy
from scrapy.http import Request
import json
import os
from pathlib import Path
from typing import Dict, Optional
import requests
from scrapy.exceptions import CloseSpider


class ImagineArtSpider(scrapy.Spider):
    name = 'imagine_art'
    
    # API é…ç½®
    api_url = 'https://imagine-blog.vyro.ai/api/video-feeds'
    base_url = 'https://imagine.animagic.art/imagine-dashboard'  # âœ… æ­£ç¡®çš„ç´ æåŸŸå
    
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'zh-CN,zh;q=0.9',
        'authorization': 'Bearer 8aef257cdbd1f6a03c0c2c3030a8a5fe3c1e3d128c95cdafa2ac71ceaf8f49af3ec23cc1fd0d443882b4297b97d901cb8d5d9f3d124afe15e51fb40c2e1b3a3b66d56bd1d17d3cb52a1a89b71fff53ca8d9c695c5e8736be498c379183d5429f519d7d79ee6265c03e060eee1d51dd58feb826bc869dd4e01299f915629cb005',
        'origin': 'https://www.imagine.art',
        'referer': 'https://www.imagine.art/',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
    }
    
    def __init__(self, target_count=50, data_manager=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_count = int(target_count)
        self.data_manager = data_manager
        self.scraped_count = 0
        self.category_name = 'ImagineArt'
        self.current_page = 1
        
        # ç¡®ä¿æœ‰ data_manager
        if not self.data_manager:
            self.logger.error("âŒ data_manager æœªæä¾›ï¼")
            raise ValueError("data_manager is required")
    
    def start_requests(self):
        """å¼€å§‹è¯·æ±‚ç¬¬ä¸€é¡µ"""
        yield self._make_request(page=1)
    
    def _make_request(self, page):
        """æ„é€  API è¯·æ±‚"""
        params = {
            'populate[category][fields][0]': '*',
            'pagination[page]': page,
            'pagination[pageSize]': 50,
        }
        
        url = self.api_url + '?' + '&'.join([f"{k}={v}" for k, v in params.items()])
        
        return scrapy.Request(
            url=url,
            headers=self.headers,
            callback=self.parse_api,
            errback=self.errback_httpbin,
            dont_filter=True,
            meta={'page': page}
        )
    
    def parse_api(self, response):
        """è§£æ API å“åº”"""
        try:
            data = json.loads(response.text)
            page = response.meta['page']
            
            self.logger.info(f"âœ… API å“åº”æˆåŠŸ: ç¬¬ {page} é¡µ")
            
            items = data.get('data', [])
            pagination = data.get('meta', {}).get('pagination', {})
            
            self.logger.info(f"   æ‰¾åˆ° {len(items)} ä¸ªä½œå“")
            self.logger.info(f"   åˆ†é¡µä¿¡æ¯: {page}/{pagination.get('pageCount', '?')}")
            
            # å¤„ç†æ¯ä¸ªä½œå“
            for item_data in items:
                if self.scraped_count >= self.target_count:
                    raise CloseSpider('Target count reached')
                
                item = self._extract_work_data(item_data)
                if item:
                    self.scraped_count += 1
                    self._process_work(item)
                    self.logger.info(f"   âœ… æå–ä½œå“ [{self.scraped_count}/{self.target_count}]")
            
            # è‡ªåŠ¨ç¿»é¡µ
            if self.scraped_count < self.target_count:
                next_page = page + 1
                page_count = pagination.get('pageCount', 0)
                
                if next_page <= page_count:
                    self.logger.info(f"   â© ç¿»é¡µåˆ°ç¬¬ {next_page} é¡µ...")
                    yield self._make_request(page=next_page)
                else:
                    self.logger.info(f"   â„¹ï¸  å·²åˆ°æœ€åä¸€é¡µ")
        
        except CloseSpider as e:
            self.logger.info(f"ğŸ çˆ¬å–å®Œæˆ: {e}")
            raise
        except json.JSONDecodeError:
            self.logger.warning(f"âš ï¸  API å“åº”ä¸æ˜¯ JSON: {response.url}")
        except Exception as e:
            self.logger.error(f"âŒ è§£æ API å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _extract_work_data(self, item_data):
        """æå–ä½œå“æ•°æ®"""
        try:
            attrs = item_data.get('attributes', {})
            
            # è·å–åˆ†ç±»
            category = attrs.get('category', {}).get('data', {}).get('attributes', {}).get('label', '')
            
            # åˆ¤æ–­ç±»å‹
            work_type = 'text2video'
            if category == 'Image to Video' or attrs.get('settings', {}).get('generated_from_image'):
                work_type = 'image2video'
            
            # æå– URL
            video_path = attrs.get('videoHd') or attrs.get('video', '')
            image_path = attrs.get('settings', {}).get('generated_from_image') or attrs.get('image', '')
            
            video_url = self.base_url + video_path if video_path else ''
            source_image_url = self.base_url + image_path if image_path and work_type == 'image2video' else ''
            cover_url = self.base_url + attrs.get('image', '') if attrs.get('image') else ''
            
            return {
                'id': str(item_data.get('id', '')),
                'prompt': attrs.get('prompt', ''),
                'video_url': video_url,
                'source_image_url': source_image_url,
                'cover_url': cover_url,
                'type': work_type,
                'media_type': 'video' if video_path.endswith('.mp4') else 'image',
            }
        
        except Exception as e:
            self.logger.error(f"æå–ä½œå“æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _process_work(self, item):
        """å¤„ç†ä½œå“ï¼šä¸‹è½½ã€ä¸Šä¼ ã€å†™å…¥TXT"""
        try:
            work_id = item['id'][:8] if item.get('id') else str(self.scraped_count)
            work_type = item['type']
            
            # ç¡®å®šä¿å­˜ç›®å½•
            if work_type == 'image2video':
                save_dir = self.data_manager.image2video_dir / self.category_name
            else:
                save_dir = self.data_manager.text2video_dir / self.category_name
            
            save_dir.mkdir(exist_ok=True, parents=True)
            
            video_s3_url = None
            cover_s3_url = None
            source_s3_url = None
            
            # ä¸‹è½½åŸå›¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if item.get('source_image_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½åŸå›¾...")
                    local_path = self._download_file(
                        item['source_image_url'],
                        save_dir / f"{work_id}_source.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            source_s3_url = s3_url
                            self.logger.info(f"    âœ… åŸå›¾ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  åŸå›¾å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½è§†é¢‘/å›¾ç‰‡
            if item.get('video_url'):
                try:
                    ext = '.mp4' if item.get('media_type') == 'video' else '.jpg'
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½{'è§†é¢‘' if ext == '.mp4' else 'å›¾ç‰‡'}...")
                    
                    local_path = self._download_file(
                        item['video_url'],
                        save_dir / f"{work_id}_video{ext}"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            video_s3_url = s3_url
                            self.logger.info(f"    âœ… {'è§†é¢‘' if ext == '.mp4' else 'å›¾ç‰‡'}ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  è§†é¢‘/å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½å°é¢
            if item.get('cover_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½å°é¢...")
                    local_path = self._download_file(
                        item['cover_url'],
                        save_dir / f"{work_id}_cover.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            cover_s3_url = s3_url
                            self.logger.info(f"    âœ… å°é¢ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  å°é¢å¤„ç†å¤±è´¥: {e}")
            
            # å†™å…¥ TXT æ–‡ä»¶
            if video_s3_url:
                self.data_manager.append_to_txt(
                    work_url=video_s3_url,
                    site_name=self.category_name,
                    source_url=source_s3_url or '',
                    prompt=item.get('prompt', ''),
                    cover_url=cover_s3_url or ''
                )
                self.logger.info(f"    âœ… å·²å†™å…¥TXTæ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _download_file(self, url, save_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return save_path
        except Exception as e:
            self.logger.error(f"      ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def errback_httpbin(self, failure):
        """é”™è¯¯å›è°ƒ"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"   åŸå› : {failure.value}")


def run_spider(data_manager, target_count=50):
    """è¿è¡Œ Scrapy Spider"""
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
    
    # é…ç½®
    settings = get_project_settings()
    settings.update({
        'LOG_LEVEL': 'INFO',
    })
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # è¿è¡Œçˆ¬è™«
    crawler = process.create_crawler(ImagineArtSpider)
    process.crawl(
        crawler,
        target_count=target_count,
        data_manager=data_manager
    )
    
    process.start()
    
    # è¿”å›çˆ¬å–æ•°é‡
    spider = crawler.spider
    return spider.scraped_count if spider else 0

