"""
Pixverse Spider - åŸºäº Scrapy æ¡†æ¶
ä¸“ä¸šçˆ¬è™«å®ç°ï¼Œç›´æ¥è°ƒç”¨ API
"""
import scrapy
from scrapy.http import Request
import json
import os
from pathlib import Path
from typing import Dict, Optional
import requests
from scrapy.exceptions import CloseSpider


class PixverseSpider(scrapy.Spider):
    name = 'pixverse'
    
    # API é…ç½®
    api_url = 'https://app-api.pixverse.ai/creative_platform/content/relation/list'
    
    headers = {
        'accept': 'application/json, text/plain, */*',
        'accept-language': 'en-US',
        'ai-anonymous-id': '19b725a3e4c4a4-02a033b10ed1b76-1c525631-3686400-19b725a3e4d1da3',
        'origin': 'https://app.pixverse.ai',
        'referer': 'https://app.pixverse.ai/',
        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/143.0.0.0 Safari/537.36',
        'x-platform': 'Web',
    }
    
    # ç±»åˆ«æ˜ å°„ï¼ˆéœ€è¦æ‰¾åˆ°å¯¹åº”çš„ secondary_category IDï¼‰
    categories = {
        'Winter Vibe': 113,
        'Ad Magic': 114,
        'Cinematic Narrative': 115,
        'Stylistic Art': 116,
        'Animal Theatre': 117,
        'Effects Rendering': 118,
        'Emotional Close-up': 119,
    }
    
    def __init__(self, target_count=20, data_manager=None, categories=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_count_per_category = int(target_count)
        self.data_manager = data_manager
        self.category_name = 'Pixverse'
        self.scraped_count = 0
        self.total_target = 0
        
        # å¦‚æœæŒ‡å®šäº†ç±»åˆ«ï¼Œåˆ™åªçˆ¬å–è¿™äº›ç±»åˆ«
        if categories:
            self.categories = {k: v for k, v in self.categories.items() if k in categories}
        
        # æ¯ä¸ªç±»åˆ«çš„è®¡æ•°å™¨
        self.category_counts = {cat: 0 for cat in self.categories.keys()}
        self.total_target = len(self.categories) * self.target_count_per_category
        
        # ç¡®ä¿æœ‰ data_manager
        if not self.data_manager:
            self.logger.error("âŒ data_manager æœªæä¾›ï¼")
            raise ValueError("data_manager is required")
    
    def start_requests(self):
        """å¼€å§‹è¯·æ±‚æ‰€æœ‰ç±»åˆ«"""
        for category_name, category_id in self.categories.items():
            self.logger.info(f"\nğŸ“‚ å¼€å§‹çˆ¬å–ç±»åˆ«: {category_name}")
            yield self._make_request(category_name, category_id, offset=0)
    
    def _make_request(self, category_name, category_id, offset):
        """æ„é€  API è¯·æ±‚"""
        params = {
            'offset': offset,
            'limit': 50,
            'primary_category': 1,
            'secondary_category': category_id,
            'platform': 'web',
            'web_offset': offset,
            'app_offset': 0,
        }
        
        url = self.api_url + '?' + '&'.join([f"{k}={v}" for k, v in params.items()])
        
        return scrapy.Request(
            url=url,
            headers=self.headers,
            callback=self.parse_api,
            errback=self.errback_httpbin,
            dont_filter=True,
            meta={
                'category_name': category_name,
                'category_id': category_id,
                'offset': offset
            }
        )
    
    def parse_api(self, response):
        """è§£æ API å“åº”"""
        try:
            data = json.loads(response.text)
            category_name = response.meta['category_name']
            category_id = response.meta['category_id']
            offset = response.meta['offset']
            
            if data.get('ErrCode') != 0:
                self.logger.error(f"âŒ API è¿”å›é”™è¯¯: {data.get('ErrMsg', 'æœªçŸ¥é”™è¯¯')}")
                return
            
            resp = data.get('Resp', {})
            items = resp.get('data', [])
            total = resp.get('total', 0)
            
            self.logger.info(f"âœ… [{category_name}] æ‰¾åˆ° {len(items)} ä¸ªä½œå“ (æ€»å…± {total})")
            
            # å¤„ç†æ¯ä¸ªä½œå“
            for item_data in items:
                # æ£€æŸ¥å½“å‰ç±»åˆ«æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡
                if self.category_counts[category_name] >= self.target_count_per_category:
                    break
                
                # æ£€æŸ¥æ€»æ•°æ˜¯å¦å·²è¾¾åˆ°ç›®æ ‡
                if self.scraped_count >= self.total_target:
                    raise CloseSpider('Target count reached')
                
                item = self._extract_work_data(item_data, category_name)
                if item:
                    self.category_counts[category_name] += 1
                    self.scraped_count += 1
                    self._process_work(item)
                    self.logger.info(
                        f"   âœ… [{category_name}] {self.category_counts[category_name]}/{self.target_count_per_category} "
                        f"(æ€»è®¡: {self.scraped_count}/{self.total_target})"
                    )
            
            # è‡ªåŠ¨ç¿»é¡µï¼ˆå¦‚æœå½“å‰ç±»åˆ«è¿˜æ²¡è¾¾åˆ°ç›®æ ‡ï¼‰
            if self.category_counts[category_name] < self.target_count_per_category:
                next_offset = offset + 50
                if next_offset < total:
                    self.logger.info(f"   â© [{category_name}] ç¿»é¡µåˆ° offset={next_offset}...")
                    yield self._make_request(category_name, category_id, next_offset)
                else:
                    self.logger.info(f"   â„¹ï¸  [{category_name}] å·²åˆ°æœ€åä¸€é¡µ")
        
        except CloseSpider as e:
            self.logger.info(f"ğŸ çˆ¬å–å®Œæˆ: {e}")
            raise
        except json.JSONDecodeError:
            self.logger.warning(f"âš ï¸  API å“åº”ä¸æ˜¯ JSON: {response.url}")
        except Exception as e:
            self.logger.error(f"âŒ è§£æ API å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _extract_work_data(self, item_data, category_name):
        """æå–ä½œå“æ•°æ®"""
        try:
            # åˆ¤æ–­ç±»å‹
            create_mode = item_data.get('create_mode', '')
            work_type = 'text2video'
            source_image_url = ''
            
            if create_mode in ['image_text', 'image']:
                work_type = 'image2video'
                # æå–åŸå›¾
                source_image_url = (
                    item_data.get('customer_img_url') or 
                    item_data.get('img_url') or 
                    item_data.get('first_frame', '')
                )
            
            video_url = item_data.get('url', '')
            cover_url = item_data.get('first_frame', '')
            prompt = item_data.get('prompt', '')
            
            return {
                'id': str(item_data.get('video_id', '')),
                'prompt': prompt,
                'video_url': video_url,
                'source_image_url': source_image_url,
                'cover_url': cover_url,
                'type': work_type,
                'category': category_name,
                'media_type': 'video',
            }
        
        except Exception as e:
            self.logger.error(f"æå–ä½œå“æ•°æ®å¤±è´¥: {e}")
            return None
    
    def _process_work(self, item):
        """å¤„ç†ä½œå“ï¼šä¸‹è½½ã€ä¸Šä¼ ã€å†™å…¥TXT"""
        try:
            work_id = item['id'][:8] if item.get('id') else str(self.scraped_count)
            work_type = item['type']
            category = item['category']
            
            # ç¡®å®šä¿å­˜ç›®å½•
            if work_type == 'image2video':
                save_dir = self.data_manager.image2video_dir / self.category_name / category
            else:
                save_dir = self.data_manager.text2video_dir / self.category_name / category
            
            save_dir.mkdir(exist_ok=True, parents=True)
            
            video_s3_url = None
            cover_s3_url = None
            source_s3_url = None
            
            # ä¸‹è½½åŸå›¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if item.get('source_image_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½åŸå›¾...")
                    local_path = self._download_file(
                        item['source_image_url'],
                        save_dir / f"{work_id}_source.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            source_s3_url = s3_url
                            self.logger.info(f"    âœ… åŸå›¾ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  åŸå›¾å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½è§†é¢‘
            if item.get('video_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½è§†é¢‘...")
                    local_path = self._download_file(
                        item['video_url'],
                        save_dir / f"{work_id}_video.mp4"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            video_s3_url = s3_url
                            self.logger.info(f"    âœ… è§†é¢‘ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  è§†é¢‘å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½å°é¢
            if item.get('cover_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½å°é¢...")
                    local_path = self._download_file(
                        item['cover_url'],
                        save_dir / f"{work_id}_cover.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            cover_s3_url = s3_url
                            self.logger.info(f"    âœ… å°é¢ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  å°é¢å¤„ç†å¤±è´¥: {e}")
            
            # å†™å…¥ TXT æ–‡ä»¶
            if video_s3_url:
                self.data_manager.append_to_txt(
                    work_url=video_s3_url,
                    site_name=self.category_name,
                    source_url=source_s3_url or '',
                    prompt=item.get('prompt', ''),
                    cover_url=cover_s3_url or ''
                )
                self.logger.info(f"    âœ… å·²å†™å…¥TXTæ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _download_file(self, url, save_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return save_path
        except Exception as e:
            self.logger.error(f"      ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def errback_httpbin(self, failure):
        """é”™è¯¯å›è°ƒ"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"   åŸå› : {failure.value}")


def run_spider(data_manager, target_count=20, categories=None):
    """è¿è¡Œ Scrapy Spider"""
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
    
    # é…ç½®
    settings = get_project_settings()
    settings.update({
        'LOG_LEVEL': 'INFO',
    })
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # è¿è¡Œçˆ¬è™«
    crawler = process.create_crawler(PixverseSpider)
    process.crawl(
        crawler,
        target_count=target_count,
        data_manager=data_manager,
        categories=categories
    )
    
    process.start()
    
    # è¿”å›çˆ¬å–æ•°é‡
    spider = crawler.spider
    return spider.scraped_count if spider else 0

