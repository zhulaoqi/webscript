"""
Wan Video Spider - åŸºäº Scrapy æ¡†æ¶
ä¸“ä¸šçˆ¬è™«å®ç°ï¼Œç»™ç½‘å€å°±èƒ½è‡ªåŠ¨çˆ¬å–
"""
import scrapy
from scrapy.http import Request
import json
import uuid
import os
from pathlib import Path
from typing import Dict, Optional
import requests


class WanVideoSpider(scrapy.Spider):
    """Wan Video çˆ¬è™« - Scrapy ç‰ˆæœ¬"""
    
    name = 'wan_video'
    allowed_domains = ['wan.video', 'wanxai.com']
    start_urls = ['https://create.wan.video/']
    
    custom_settings = {
        # ä¸‹è½½å»¶è¿Ÿï¼ˆç¤¼è²Œçˆ¬å–ï¼‰
        'DOWNLOAD_DELAY': 2,
        'RANDOMIZE_DOWNLOAD_DELAY': True,
        
        # å¹¶å‘è®¾ç½®
        'CONCURRENT_REQUESTS': 4,
        'CONCURRENT_REQUESTS_PER_DOMAIN': 2,
        
        # é‡è¯•è®¾ç½®
        'RETRY_TIMES': 3,
        'RETRY_HTTP_CODES': [500, 502, 503, 504, 408, 429],
        
        # User-Agent
        'USER_AGENT': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
        
        # æ—¥å¿—çº§åˆ«
        'LOG_LEVEL': 'INFO',
        
        # è‡ªåŠ¨é™é€Ÿ
        'AUTOTHROTTLE_ENABLED': True,
        'AUTOTHROTTLE_START_DELAY': 1,
        'AUTOTHROTTLE_MAX_DELAY': 10,
        
        # ä¸­é—´ä»¶
        'DOWNLOADER_MIDDLEWARES': {
            'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware': None,
            'scrapy_user_agents.middlewares.RandomUserAgentMiddleware': 400,
        }
    }
    
    def __init__(self, target_count=50, data_manager=None, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.target_count = int(target_count)
        self.data_manager = data_manager
        self.scraped_count = 0
        self.category_name = 'WanVideo'  # å»æ‰ç©ºæ ¼
        
        # ç¡®ä¿æœ‰ data_manager
        if not self.data_manager:
            self.logger.error("âŒ data_manager æœªæä¾›ï¼")
            raise ValueError("data_manager is required")
    
    def parse(self, response):
        """è§£æé¦–é¡µï¼Œç›´æ¥è°ƒç”¨çœŸå® API"""
        self.logger.info(f"ğŸ¯ ä½¿ç”¨çœŸå® API è·å–ä½œå“åˆ—è¡¨")
        
        # çœŸå®çš„ API åœ°å€ï¼ˆä» Network åˆ†æå¾—åˆ°ï¼‰
        api_url = 'https://create.wan.video/wanx/api/v2/square/recommend'
        
        # è¯·æ±‚å‚æ•°
        payload = {
            'pageSize': self.target_count,
            'source': 'task_image',
            'mediaType': 'all',
            'token': ''  # ç¬¬ä¸€é¡µä¸ºç©ºï¼Œåç»­åˆ†é¡µä¼šç”¨åˆ°
        }
        
        # å‘é€ POST è¯·æ±‚
        yield Request(
            url=api_url,
            method='POST',
            headers={
                'Content-Type': 'application/json',
                'Accept': 'application/json',
                'x-platform': 'web',
            },
            body=json.dumps(payload),
            callback=self.parse_api,
            errback=self.errback_httpbin
        )
    
    def parse_api(self, response):
        """è§£æçœŸå® API å“åº”"""
        try:
            data = json.loads(response.text)
            
            if not data.get('success'):
                self.logger.error(f"âŒ API è¿”å›å¤±è´¥: {data.get('errorMsg', 'æœªçŸ¥é”™è¯¯')}")
                return
            
            # æå–ä½œå“åˆ—è¡¨
            works = data.get('data', {}).get('works', [])
            
            if not works:
                self.logger.warning(f"âš ï¸  æœªæ‰¾åˆ°ä½œå“æ•°æ®")
                return
            
            self.logger.info(f"âœ… è·å–åˆ° {len(works)} ä¸ªä½œå“")
            
            # éå†æ¯ä¸ªä½œå“
            for work_item in works:
                if work_item.get('type') != 'WORK':
                    continue
                
                work_data = work_item.get('data', {})
                
                # æå–å…³é”®ä¿¡æ¯
                media_type = work_data.get('mediaType')  # "video" æˆ– "image"
                task_type = work_data.get('taskType')    # "text_to_video", "image_to_video" ç­‰
                task_input = work_data.get('taskInput', {})
                image_info = work_data.get('image', {})
                
                # æ„é€ æ ‡å‡†åŒ–æ•°æ®
                item = {
                    'id': work_data.get('resourceId'),
                    'type': 'image2video' if 'image_to' in task_type else 'text2video',
                    'media_type': media_type,
                    'prompt': task_input.get('prompt') or task_input.get('finalPrompt', 'No prompt'),
                    
                    # è§†é¢‘/å›¾ç‰‡ URL
                    'video_url': image_info.get('downloadUrl') if media_type == 'video' else image_info.get('url'),
                    'cover_url': image_info.get('resizeUrl') or image_info.get('url'),
                    
                    # åŸå›¾ URLï¼ˆå›¾ç”Ÿè§†é¢‘æ‰æœ‰ï¼‰
                    'source_image_url': None,
                }
                
                # æå–åŸå›¾ï¼ˆå¦‚æœæœ‰ï¼‰
                ref_images = task_input.get('refImagesurlsInfo', [])
                if ref_images and len(ref_images) > 0:
                    item['source_image_url'] = ref_images[0].get('originImage')
                    item['type'] = 'image2video'
                
                # è®¡æ•°æ§åˆ¶
                if self.scraped_count >= self.target_count:
                    self.logger.info(f"âœ… å·²è¾¾åˆ°ç›®æ ‡æ•°é‡: {self.target_count}")
                    return
                
                self.scraped_count += 1
                self.logger.info(f"  [{self.scraped_count}/{self.target_count}] {item['type']} - {item['prompt'][:50]}...")
                
                # ä¸‹è½½å¹¶ä¸Šä¼ åˆ° S3
                self._process_work(item)
                
                yield item
            
            # åˆ†é¡µï¼šè·å–ä¸‹ä¸€é¡µ
            next_token = data.get('data', {}).get('token')
            if next_token and self.scraped_count < self.target_count:
                self.logger.info(f"ğŸ“„ ç»§ç»­è·å–ä¸‹ä¸€é¡µ...")
                
                payload = {
                    'pageSize': self.target_count - self.scraped_count,
                    'source': 'task_image',
                    'mediaType': 'all',
                    'token': next_token
                }
                
                yield Request(
                    url='https://create.wan.video/wanx/api/v2/square/recommend',
                    method='POST',
                    headers={
                        'Content-Type': 'application/json',
                        'Accept': 'application/json',
                        'x-platform': 'web',
                    },
                    body=json.dumps(payload),
                    callback=self.parse_api,
                    dont_filter=True
                )
                
        except json.JSONDecodeError:
            self.logger.warning(f"âš ï¸  API å“åº”ä¸æ˜¯ JSON")
        except Exception as e:
            self.logger.error(f"âŒ è§£æ API å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def parse_work(self, response):
        """è§£æä½œå“è¯¦æƒ…é¡µ"""
        self.logger.info(f"ğŸ“„ è§£æä½œå“: {response.url}")
        
        # æå–è§†é¢‘URL
        video_urls = response.css('video source::attr(src), video::attr(src)').getall()
        
        # æå–å›¾ç‰‡URL
        image_urls = response.css('img[src*="wanxai.com"]::attr(src)').getall()
        
        # æå–æç¤ºè¯
        prompt = response.css('.prompt::text, [class*="prompt"]::text').get() or 'No prompt'
        
        if video_urls or image_urls:
            yield {
                'url': response.url,
                'video_url': video_urls[0] if video_urls else None,
                'cover_url': image_urls[0] if image_urls else None,
                'prompt': prompt.strip(),
                'type': 'text2video',
            }
    
    def _extract_json_from_script(self, script_text: str) -> Optional[Dict]:
        """ä» <script> æ ‡ç­¾ä¸­æå– JSON"""
        try:
            # å°è¯•æå– window.__INITIAL_STATE__ æˆ–ç±»ä¼¼å˜é‡
            import re
            
            patterns = [
                r'window\.__INITIAL_STATE__\s*=\s*({.+?});',
                r'window\.__NEXT_DATA__\s*=\s*({.+?})</script>',
                r'var\s+\w+\s*=\s*({.+?});',
            ]
            
            for pattern in patterns:
                match = re.search(pattern, script_text, re.DOTALL)
                if match:
                    json_str = match.group(1)
                    return json.loads(json_str)
            
            return None
        except:
            return None
    
    def _parse_json_data(self, data: Dict) -> list:
        """ä» JSON ä¸­æå–ä½œå“åˆ—è¡¨"""
        # é€’å½’æŸ¥æ‰¾æ•°ç»„æ•°æ®
        def find_arrays(obj, depth=0):
            if depth > 5:  # é™åˆ¶é€’å½’æ·±åº¦
                return []
            
            if isinstance(obj, list) and len(obj) > 0:
                # æ£€æŸ¥æ˜¯å¦æ˜¯ä½œå“æ•°ç»„ï¼ˆåŒ…å« url/video ç­‰å­—æ®µï¼‰
                if isinstance(obj[0], dict):
                    keys = obj[0].keys()
                    if any(k in keys for k in ['url', 'video', 'id', 'src']):
                        return obj
            
            if isinstance(obj, dict):
                for value in obj.values():
                    result = find_arrays(value, depth + 1)
                    if result:
                        return result
            
            return []
        
        return find_arrays(data)
    
    def _create_work_item(self, work: Dict) -> Dict:
        """åˆ›å»ºæ ‡å‡†åŒ–çš„ä½œå“æ•°æ®"""
        # æ™ºèƒ½æå–å­—æ®µï¼ˆé€‚é…ä¸åŒçš„ API å“åº”æ ¼å¼ï¼‰
        def get_value(data, keys):
            for key in keys:
                if key in data and data[key]:
                    return data[key]
            return None
        
        return {
            'id': get_value(work, ['id', 'video_id', '_id']),
            'video_url': get_value(work, ['video_url', 'videoUrl', 'url', 'video', 'media_url']),
            'cover_url': get_value(work, ['cover_url', 'coverUrl', 'cover', 'thumbnail', 'poster']),
            'prompt': get_value(work, ['prompt', 'description', 'text', 'caption']) or 'No prompt',
            'source_image_url': get_value(work, ['source_image_url', 'sourceImageUrl', 'source', 'input_image']),
            'type': 'image2video' if get_value(work, ['source_image_url', 'source']) else 'text2video',
        }
    
    def _process_work(self, item):
        """å¤„ç†ä½œå“ï¼šä¸‹è½½ã€ä¸Šä¼ ã€å†™å…¥TXT"""
        try:
            
            work_id = item['id'][:8] if item.get('id') else str(self.scraped_count)
            work_type = item['type']
            
            # ç¡®å®šä¿å­˜ç›®å½•
            if work_type == 'image2video':
                save_dir = self.data_manager.image2video_dir / 'wan_video'
            else:
                save_dir = self.data_manager.text2video_dir / 'wan_video'
            
            save_dir.mkdir(exist_ok=True, parents=True)
            
            video_s3_url = None
            cover_s3_url = None
            source_s3_url = None
            
            # ä¸‹è½½åŸå›¾ï¼ˆå¦‚æœæœ‰ï¼‰
            if item.get('source_image_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½åŸå›¾...")
                    local_path = self._download_file(
                        item['source_image_url'],
                        save_dir / f"{work_id}_source.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            source_s3_url = s3_url
                            self.logger.info(f"    âœ… åŸå›¾ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  åŸå›¾å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½è§†é¢‘/å›¾ç‰‡
            if item.get('video_url'):
                try:
                    ext = '.mp4' if item.get('media_type') == 'video' else '.jpg'
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½{'è§†é¢‘' if ext == '.mp4' else 'å›¾ç‰‡'}...")
                    
                    local_path = self._download_file(
                        item['video_url'],
                        save_dir / f"{work_id}_video{ext}"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            video_s3_url = s3_url
                            self.logger.info(f"    âœ… {'è§†é¢‘' if ext == '.mp4' else 'å›¾ç‰‡'}ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  è§†é¢‘/å›¾ç‰‡å¤„ç†å¤±è´¥: {e}")
            
            # ä¸‹è½½å°é¢
            if item.get('cover_url'):
                try:
                    self.logger.info(f"    ğŸ“¥ ä¸‹è½½å°é¢...")
                    local_path = self._download_file(
                        item['cover_url'],
                        save_dir / f"{work_id}_cover.jpg"
                    )
                    if local_path:
                        s3_url = self.data_manager.upload_to_s3(
                            str(local_path), '', os.path.basename(str(local_path)))
                        if s3_url:
                            cover_s3_url = s3_url
                            self.logger.info(f"    âœ… å°é¢ä¸Šä¼ æˆåŠŸ")
                except Exception as e:
                    self.logger.warning(f"    âš ï¸  å°é¢å¤„ç†å¤±è´¥: {e}")
            
            # å†™å…¥ TXT æ–‡ä»¶
            if video_s3_url:
                self.data_manager.append_to_txt(
                    work_url=video_s3_url,
                    site_name=self.category_name,
                    source_url=source_s3_url or '',
                    prompt=item.get('prompt', ''),
                    cover_url=cover_s3_url or ''
                )
                self.logger.info(f"    âœ… å·²å†™å…¥TXTæ–‡ä»¶")
            
        except Exception as e:
            self.logger.error(f"    âŒ å¤„ç†å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
    
    def _download_file(self, url, save_path):
        """ä¸‹è½½æ–‡ä»¶"""
        try:
            response = requests.get(url, timeout=60, stream=True)
            response.raise_for_status()
            
            with open(save_path, 'wb') as f:
                for chunk in response.iter_content(chunk_size=8192):
                    f.write(chunk)
            
            return save_path
        except Exception as e:
            self.logger.error(f"      ä¸‹è½½å¤±è´¥: {e}")
            return None
    
    def errback_httpbin(self, failure):
        """é”™è¯¯å›è°ƒ"""
        self.logger.error(f"âŒ è¯·æ±‚å¤±è´¥: {failure.request.url}")
        self.logger.error(f"   åŸå› : {failure.value}")


def run_spider(data_manager, target_count=50):
    """è¿è¡Œ Scrapy Spider"""
    from scrapy.crawler import CrawlerProcess
    from scrapy.utils.project import get_project_settings
    
    # é…ç½®
    settings = get_project_settings()
    settings.update({
        'LOG_LEVEL': 'INFO',
    })
    
    # åˆ›å»ºçˆ¬è™«è¿›ç¨‹
    process = CrawlerProcess(settings)
    
    # è¿è¡Œçˆ¬è™«
    crawler = process.create_crawler(WanVideoSpider)
    process.crawl(
        crawler,
        target_count=target_count,
        data_manager=data_manager
    )
    
    process.start()
    
    # è¿”å›çˆ¬å–æ•°é‡
    spider = crawler.spider
    return spider.scraped_count if spider else 0

