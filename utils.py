"""
å·¥å…·å‡½æ•°
"""
import os
import time
import random
import requests
import zipfile
from pathlib import Path
from typing import Dict, List, Optional
from urllib.parse import urlparse
import json
from tqdm import tqdm
import boto3
from botocore.exceptions import ClientError
from config import DOWNLOAD_CONFIG, USER_AGENTS, AWS_S3_CONFIG


class S3Uploader:
    """S3ä¸Šä¼ å·¥å…·ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ–S3å®¢æˆ·ç«¯"""
        self.s3_client = boto3.client(
            's3',
            aws_access_key_id=AWS_S3_CONFIG['access_key_id'],
            aws_secret_access_key=AWS_S3_CONFIG['secret_access_key'],
            region_name=AWS_S3_CONFIG['region']
        )
        self.bucket_name = AWS_S3_CONFIG['bucket_name']
        self.cdn_prefix = AWS_S3_CONFIG['url_prefix']
    
    def upload_file(self, local_path: str, s3_key: str) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°S3
        
        Args:
            local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            s3_key: S3å¯¹è±¡é”®å
            
        Returns:
            CDN URLæˆ–None
        """
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(local_path):
                print(f"âš ï¸  æœ¬åœ°æ–‡ä»¶ä¸å­˜åœ¨: {local_path}")
                return None
            
            # ç¡®å®šContent-Type
            content_type = self._get_content_type(local_path)
            
            print(f"    ğŸ“¤ ä¸Šä¼ ä¸­: {os.path.basename(local_path)} -> S3")
            
            # ä¸Šä¼ æ–‡ä»¶ï¼ˆä¸ä½¿ç”¨ACLï¼Œå­˜å‚¨æ¡¶å·²é…ç½®ä¸ºå…¬å¼€è®¿é—®ï¼‰
            self.s3_client.upload_file(
                local_path,
                self.bucket_name,
                s3_key,
                ExtraArgs={
                    'ContentType': content_type
                }
            )
            
            # è¿”å›CDN URL
            cdn_url = f"{self.cdn_prefix}{s3_key}"
            print(f"    âœ… S3æˆåŠŸ: {cdn_url}")
            return cdn_url
            
        except ClientError as e:
            print(f"    âŒ S3ä¸Šä¼ å¤±è´¥: {e}")
            return None
        except Exception as e:
            print(f"    âŒ ä¸Šä¼ é”™è¯¯: {e}")
            return None
    
    @staticmethod
    def _get_content_type(file_path: str) -> str:
        """æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–Content-Type"""
        ext = os.path.splitext(file_path)[1].lower()
        content_types = {
            '.mp4': 'video/mp4',
            '.mov': 'video/quicktime',
            '.avi': 'video/x-msvideo',
            '.jpg': 'image/jpeg',
            '.jpeg': 'image/jpeg',
            '.png': 'image/png',
            '.gif': 'image/gif',
            '.webp': 'image/webp',
        }
        return content_types.get(ext, 'application/octet-stream')


class DownloadUtils:
    """ä¸‹è½½å·¥å…·ç±»"""
    
    @staticmethod
    def get_random_user_agent() -> str:
        """è·å–éšæœºUser-Agent"""
        return random.choice(USER_AGENTS)
    
    @staticmethod
    def random_delay():
        """éšæœºå»¶è¿Ÿ"""
        delay = random.uniform(
            DOWNLOAD_CONFIG['delay_min'], 
            DOWNLOAD_CONFIG['delay_max']
        )
        time.sleep(delay)
    
    @staticmethod
    def download_file(url: str, save_path: str, proxies: Optional[Dict] = None, referer: str = None) -> bool:
        """
        ä¸‹è½½æ–‡ä»¶ï¼ˆæ”¯æŒé˜²ç›—é“¾çªç ´ï¼‰
        
        Args:
            url: æ–‡ä»¶URL
            save_path: ä¿å­˜è·¯å¾„
            proxies: ä»£ç†é…ç½®
            referer: æ¥æºé¡µé¢ï¼ˆç”¨äºçªç ´é˜²ç›—é“¾ï¼‰
            
        Returns:
            æ˜¯å¦ä¸‹è½½æˆåŠŸ
        """
        # è·³è¿‡blobå’Œdata URLs
        if url.startswith('blob:') or url.startswith('data:'):
            return False
        
        # è·³è¿‡ç”¨æˆ·å¤´åƒã€å›¾æ ‡ç­‰éç´ æå†…å®¹
        skip_patterns = [
            'profile-image', 'avatar', 'user-avatar', 'user_avatar',
            'favicon', 'logo', 'icon', 'thumbnail_placeholder',
            '/users/', '/user/', '/profile/', '/creator/',
        ]
        url_lower = url.lower()
        for pattern in skip_patterns:
            if pattern in url_lower:
                return False
        
        max_retries = DOWNLOAD_CONFIG['max_retries']
        timeout = 60  # å¢åŠ è¶…æ—¶æ—¶é—´åˆ°60ç§’
        
        # æ ¹æ®URLæ¨æ–­æ¥æºç½‘ç«™
        if not referer:
            from urllib.parse import urlparse
            parsed = urlparse(url)
            referer = f"{parsed.scheme}://{parsed.netloc}/"
        
        for attempt in range(max_retries):
            try:
                # å¢å¼ºè¯·æ±‚å¤´ï¼Œæ¨¡æ‹ŸçœŸå®æµè§ˆå™¨
                headers = {
                    'User-Agent': DownloadUtils.get_random_user_agent(),
                    'Referer': referer,  # ä½¿ç”¨æ­£ç¡®çš„æ¥æºé¡µé¢
                    'Accept': '*/*',
                    'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                    'Accept-Encoding': 'gzip, deflate, br',
                    'Connection': 'keep-alive',
                    'Sec-Fetch-Dest': 'video' if '.mp4' in url or '.mov' in url else 'image',
                    'Sec-Fetch-Mode': 'no-cors',
                    'Sec-Fetch-Site': 'same-origin',
                }
                
                # åŠ¨æ€è¶…æ—¶ï¼šè¿æ¥è¶…æ—¶15ç§’ï¼Œè¯»å–è¶…æ—¶60ç§’
                response = requests.get(
                    url, 
                    headers=headers, 
                    proxies=proxies,
                    timeout=(15, 60),  # (connect timeout, read timeout)
                    stream=True,
                    allow_redirects=True  # å…è®¸é‡å®šå‘
                )
                response.raise_for_status()
                
                # æ£€æŸ¥Content-Typeï¼Œç¡®ä¿ä¸æ˜¯HTMLé”™è¯¯é¡µ
                content_type = response.headers.get('content-type', '').lower()
                if 'text/html' in content_type:
                    return False
                
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                os.makedirs(os.path.dirname(save_path), exist_ok=True)
                
                # ä¸‹è½½æ–‡ä»¶
                total_size = int(response.headers.get('content-length', 0))
                downloaded_size = 0
                
                with open(save_path, 'wb') as f:
                    if total_size == 0:
                        content = response.content
                        f.write(content)
                        downloaded_size = len(content)
                    else:
                        for chunk in response.iter_content(chunk_size=8192):
                            if chunk:
                                f.write(chunk)
                                downloaded_size += len(chunk)
                
                # éªŒè¯ä¸‹è½½å®Œæ•´æ€§
                if total_size > 0 and downloaded_size < total_size * 0.9:
                    if attempt < max_retries - 1:
                        time.sleep(2 ** attempt)
                        continue
                    return False
                
                # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼ˆè‡³å°‘1KBï¼Œé¿å…ä¸‹è½½åˆ°é”™è¯¯é¡µé¢ï¼‰
                if downloaded_size < 1024:
                    return False
                
                return True
                
            except requests.exceptions.Timeout as e:
                # è¶…æ—¶é”™è¯¯ï¼Œé™é»˜é‡è¯•
                if attempt < max_retries - 1:
                    time.sleep(3)  # ç­‰å¾…3ç§’åé‡è¯•
                    continue
                return False
            except requests.exceptions.ConnectionError as e:
                # è¿æ¥é”™è¯¯ï¼Œé™é»˜é‡è¯•
                if attempt < max_retries - 1:
                    time.sleep(3)
                    continue
                return False
            except Exception as e:
                # å…¶ä»–é”™è¯¯
                if attempt < max_retries - 1:
                    time.sleep(2 ** attempt)  # æŒ‡æ•°é€€é¿
                    continue
                return False
                    
        return False
    
    @staticmethod
    def get_file_extension(url: str) -> str:
        """ä»URLè·å–æ–‡ä»¶æ‰©å±•å"""
        parsed = urlparse(url)
        path = parsed.path
        ext = os.path.splitext(path)[1]
        if not ext:
            # å°è¯•ä»æŸ¥è¯¢å‚æ•°è·å–
            if 'format=' in url:
                ext = '.' + url.split('format=')[1].split('&')[0]
            else:
                ext = '.mp4'  # é»˜è®¤è§†é¢‘æ ¼å¼
        return ext


class DataManager:
    """æ•°æ®ç®¡ç†ç±»"""
    
    def __init__(self, output_dir: str, use_s3: bool = True):
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # åˆ›å»ºå­ç›®å½•
        self.text2video_dir = self.output_dir / 'text2video'
        self.image2video_dir = self.output_dir / 'image2video'
        self.text2video_dir.mkdir(exist_ok=True)
        self.image2video_dir.mkdir(exist_ok=True)
        
        # æ•°æ®å­˜å‚¨
        self.text2video_data: List[Dict] = []
        self.image2video_data: List[Dict] = []
        
        # S3ä¸Šä¼ å™¨
        self.use_s3 = use_s3
        if use_s3:
            self.s3_uploader = S3Uploader()
            print("âœ“ S3ä¸Šä¼ å·²å¯ç”¨")
    
    def upload_to_s3(self, local_path: str, category: str, filename: str) -> Optional[str]:
        """
        ä¸Šä¼ æ–‡ä»¶åˆ°S3å¹¶è¿”å›URL
        
        Args:
            local_path: æœ¬åœ°æ–‡ä»¶è·¯å¾„
            category: åˆ†ç±»ï¼ˆç”¨äºS3è·¯å¾„ï¼‰
            filename: æ–‡ä»¶å
            
        Returns:
            S3 CDN URL
        """
        if not self.use_s3 or not local_path or not os.path.exists(local_path):
            return None
        
        # ç”ŸæˆS3é”®å: video-materials/æ–‡ä»¶å (ä¸å†åŒ…å«ç½‘ç«™åˆ†ç±»)
        if category:
            s3_key = f"video-materials/{category}/{filename}"
        else:
            s3_key = f"video-materials/{filename}"
        
        # ä¸Šä¼ å¹¶è·å–URL
        cdn_url = self.s3_uploader.upload_file(local_path, s3_key)
        
        return cdn_url
    
    def add_text2video(self, data: Dict):
        """æ·»åŠ æ–‡ç”Ÿè§†é¢‘æ•°æ®"""
        self.text2video_data.append(data)
        print(f"    ğŸ’¾ æ•°æ®å·²ä¿å­˜ (æ€»è®¡: {len(self.text2video_data)} æ¡)")
    
    def add_image2video(self, data: Dict):
        """æ·»åŠ å›¾ç”Ÿè§†é¢‘æ•°æ®"""
        self.image2video_data.append(data)
        print(f"    ğŸ’¾ æ•°æ®å·²ä¿å­˜ (æ€»è®¡: {len(self.image2video_data)} æ¡)")
    
    def save_json(self):
        """ä¿å­˜JSONæ•°æ®ï¼ˆä»…ç”¨äºè°ƒè¯•ï¼‰"""
        pass  # ä¸å†è¾“å‡ºJSONæ–‡ä»¶
    
    def create_zip(self):
        """åˆ›å»ºZIPå‹ç¼©åŒ…"""
        pass  # ä¸å†åˆ›å»ºZIP
    
    def append_to_txt(self, work_url: str, site_name: str, source_url: str = '', prompt: str = '', cover_url: str = ''):
        """
        å®æ—¶è¿½åŠ æ•°æ®åˆ°TXTæ–‡ä»¶
        æ ¼å¼ï¼šä½œå“URL åŸå›¾URL æç¤ºè¯ ç¼©ç•¥å›¾URLï¼ˆå›ºå®š4åˆ—ï¼Œç©ºæ ¼åˆ†éš”ï¼‰
        
        Args:
            work_url: ä½œå“URLï¼ˆè§†é¢‘æˆ–å›¾ç‰‡ï¼‰
            site_name: ç½‘ç«™åç§°
            source_url: åŸå›¾URLï¼ˆå›¾ç”Ÿè§†é¢‘/å›¾ç”Ÿå›¾çš„è¾“å…¥å›¾ï¼‰
            prompt: æç¤ºè¯
            cover_url: ç¼©ç•¥å›¾URLï¼ˆè§†é¢‘å°é¢ï¼‰
        """
        try:
            # æ¸…ç†æç¤ºè¯ï¼ˆå»æ‰æ¢è¡Œç¬¦ï¼Œé™åˆ¶é•¿åº¦ï¼‰
            if prompt:
                prompt = prompt.replace('\n', ' ').replace('\r', ' ').replace('\t', ' ')
                # å»é™¤å¤šä½™ç©ºæ ¼
                prompt = ' '.join(prompt.split())
                # é™åˆ¶é•¿åº¦
                if len(prompt) > 500:
                    prompt = prompt[:500]
            
            # å›ºå®š4åˆ—æ ¼å¼
            line = f"{work_url} {source_url or 'æ— åŸå›¾'} {prompt or 'æ— æç¤ºè¯'} {cover_url or 'æ— ç¼©ç•¥å›¾'}\n"
            
            # ç½‘ç«™ä¸“ç”¨æ–‡ä»¶
            site_normalized = site_name.lower().replace(' ', '_').replace('.', '_')
            site_txt_path = self.output_dir.parent / f'{site_normalized}.txt'
            
            # è¿½åŠ åˆ°ç½‘ç«™æ–‡ä»¶
            with open(site_txt_path, 'a', encoding='utf-8') as f:
                f.write(line)
            
            # è¿½åŠ åˆ°æ€»æ–‡ä»¶
            all_txt_path = self.output_dir.parent / 'all_materials.txt'
            with open(all_txt_path, 'a', encoding='utf-8') as f:
                f.write(line)
                
        except Exception as e:
            print(f"  âš ï¸  å†™å…¥TXTå¤±è´¥: {e}")
    
    def export_txt(self, site_name: str = None):
        """
        å¯¼å‡ºTXTæ–‡ä»¶ - æ¯è¡Œä¸€ä¸ªS3 URLï¼Œæ•ˆç‡æ›´é«˜ï¼ˆå·²åºŸå¼ƒï¼Œæ”¹ç”¨å®æ—¶å†™å…¥ï¼‰
        
        Args:
            site_name: ç½‘ç«™åç§°ï¼Œå¦‚æœæä¾›åˆ™åªå¯¼å‡ºè¯¥ç½‘ç«™æ•°æ®
        """
        try:
            print(f"\n  ğŸ“ å¯¼å‡ºTXT: site_name={site_name}")
            print(f"  ğŸ“ æ•°æ®ç»Ÿè®¡: text2video={len(self.text2video_data)}, image2video={len(self.image2video_data)}")
            
            # æ”¶é›†æ‰€æœ‰URL
            all_urls = []
            
            # æ·»åŠ æ–‡ç”Ÿè§†é¢‘æ•°æ®çš„URL
            for item in self.text2video_data:
                category = item.get('category', '')
                
                # å¦‚æœæŒ‡å®šäº†site_nameï¼Œåªå¯¼å‡ºåŒ¹é…çš„æ•°æ®
                if site_name:
                    site_normalized = site_name.lower().replace('_', ' ')
                    category_normalized = category.lower().replace('_', ' ')
                    if site_normalized not in category_normalized and category_normalized not in site_normalized:
                        continue
                
                # æ·»åŠ è§†é¢‘URL
                if item.get('video_s3_url'):
                    all_urls.append(item.get('video_s3_url'))
                
                # æ·»åŠ ç¼©ç•¥å›¾URL
                if item.get('thumbnail_s3_url'):
                    all_urls.append(item.get('thumbnail_s3_url'))
            
            # æ·»åŠ å›¾ç”Ÿè§†é¢‘æ•°æ®çš„URL
            for item in self.image2video_data:
                category = item.get('category', '')
                
                # å¦‚æœæŒ‡å®šäº†site_nameï¼Œåªå¯¼å‡ºåŒ¹é…çš„æ•°æ®
                if site_name:
                    site_normalized = site_name.lower().replace('_', ' ')
                    category_normalized = category.lower().replace('_', ' ')
                    if site_normalized not in category_normalized and category_normalized not in site_normalized:
                        continue
                
                # æ·»åŠ åŸå›¾URL
                if item.get('source_image_s3_url'):
                    all_urls.append(item.get('source_image_s3_url'))
                
                # æ·»åŠ è§†é¢‘URL
                if item.get('video_s3_url'):
                    all_urls.append(item.get('video_s3_url'))
                
                # æ·»åŠ ç¼©ç•¥å›¾URL
                if item.get('thumbnail_s3_url'):
                    all_urls.append(item.get('thumbnail_s3_url'))
            
            print(f"  ğŸ”— æ”¶é›†åˆ° {len(all_urls)} ä¸ªS3é“¾æ¥")
            
            if not all_urls:
                print(f"  âš ï¸  æ²¡æœ‰æ•°æ®å¯å¯¼å‡º (site: {site_name})")
                return None
            
            # ç”Ÿæˆæ–‡ä»¶å
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            if site_name:
                filename = f'{site_name}_{timestamp}.txt'
            else:
                filename = f'all_materials_{timestamp}.txt'
            
            txt_path = self.output_dir.parent / filename
            
            # å†™å…¥TXTæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªURLï¼‰
            with open(txt_path, 'w', encoding='utf-8') as f:
                for url in all_urls:
                    f.write(url + '\n')
            
            print(f"  âœ… TXTå·²å¯¼å‡º: {txt_path}")
            print(f"  ğŸ“Š S3é“¾æ¥æ•°: {len(all_urls)} æ¡")
            return txt_path
            
        except Exception as e:
            print(f"âœ— å¯¼å‡ºTXTå¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return None
    
    def get_summary(self) -> Dict:
        """è·å–æ•°æ®æ‘˜è¦"""
        return {
            'text2video_count': len(self.text2video_data),
            'image2video_count': len(self.image2video_data),
            'total_count': len(self.text2video_data) + len(self.image2video_data)
        }


def setup_proxy(proxy_config: Dict) -> Optional[Dict]:
    """
    è®¾ç½®ä»£ç†
    
    Args:
        proxy_config: ä»£ç†é…ç½®
        
    Returns:
        requestsä»£ç†å­—å…¸
    """
    if not proxy_config.get('host'):
        return None
    
    if proxy_config.get('user') and proxy_config.get('password'):
        proxy_url = f"http://{proxy_config['user']}:{proxy_config['password']}@{proxy_config['host']}"
        if proxy_config.get('port'):
            proxy_url += f":{proxy_config['port']}"
    else:
        proxy_url = f"http://{proxy_config['host']}"
        if proxy_config.get('port'):
            proxy_url += f":{proxy_config['port']}"
    
    return {
        'http': proxy_url,
        'https': proxy_url
    }

